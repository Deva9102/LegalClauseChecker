{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHMCNZSGS7H6"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8987,
     "status": "ok",
     "timestamp": 1755089486189,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "9e128D3SS93E",
    "outputId": "87810f6d-27c3-40e4-d300-3e749ab6eff5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load saved files\n",
    "df_labeled = pd.read_pickle(\"/content/drive/MyDrive/data_cuad_transformer/df_labeled.pkl\")\n",
    "\n",
    "with open(\"/content/drive/MyDrive/data_cuad_transformer/label_names.pkl\", \"rb\") as f:\n",
    "    label_names = pickle.load(f)\n",
    "\n",
    "pos_weight = np.load(\"/content/drive/MyDrive/data_cuad_transformer/pos_weight.npy\")\n",
    "\n",
    "print(\"Loaded df_labeled, label_names, pos_weight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acvvYvWHWHcr"
   },
   "source": [
    "### BERT Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1755089580197,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "9xD_6FR9PZCI",
    "outputId": "e24e7314-4269-4169-99c1-711f89aebc7f"
   },
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd, pickle, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "df_labeled = pd.read_pickle(\"/content/drive/MyDrive/data_cuad_transformer/df_labeled.pkl\")\n",
    "with open(\"/content/drive/MyDrive/data_cuad_transformer/label_names.pkl\", \"rb\") as f:\n",
    "    label_names = pickle.load(f)\n",
    "\n",
    "df_labeled[\"y_multi\"] = df_labeled[\"y_multi\"].apply(lambda v: np.asarray(v, dtype=np.float32))\n",
    "y_lengths = df_labeled[\"y_multi\"].map(lambda v: v.shape[0]).unique()\n",
    "if len(y_lengths) != 1:\n",
    "    raise ValueError(f\"Inconsistent y_multi lengths across rows: {y_lengths}\")\n",
    "TRUE_NUM_LABELS = int(y_lengths[0])\n",
    "\n",
    "if 'label_names' in globals():\n",
    "    if len(label_names) != TRUE_NUM_LABELS:\n",
    "        print(f\"[warn] label_names len={len(label_names)}\")\n",
    "        if len(label_names) > TRUE_NUM_LABELS:\n",
    "            label_names = list(label_names)[:TRUE_NUM_LABELS]\n",
    "        else:\n",
    "            label_names = list(label_names) + [f\"label_{i}\" for i in range(len(label_names), TRUE_NUM_LABELS)]\n",
    "\n",
    "NUM_LABELS = TRUE_NUM_LABELS\n",
    "print(\"NUM_LABELS:\", NUM_LABELS)\n",
    "\n",
    "def align_matrix(mat, num_labels):\n",
    "    mat = np.asarray(mat, dtype=np.float32)\n",
    "    if mat.ndim == 1:\n",
    "        mat = mat[None, :]\n",
    "    if mat.shape[1] > num_labels:\n",
    "        return mat[:, :num_labels]\n",
    "    if mat.shape[1] < num_labels:\n",
    "        pad = np.zeros((mat.shape[0], num_labels - mat.shape[1]), dtype=np.float32)\n",
    "        return np.hstack([mat, pad])\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1755090536389,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "5wwGzvXgQ67E"
   },
   "outputs": [],
   "source": [
    "class ClauseDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512, num_labels=None):\n",
    "        self.texts = list(texts)\n",
    "        arr = np.array(list(labels), dtype=np.float32)\n",
    "        if num_labels is not None:\n",
    "            arr = align_matrix(arr, num_labels)\n",
    "        self.labels = arr\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "aa337951aa3b427aa14038a0359859fe",
      "e6685670a08143578e3ba062abd2509a",
      "40e29f8556194ce4a86ea286f2a5f40b",
      "9cf6f0ecf2094425b487af5f7d0cf462",
      "a7175696d4f9484f83a3fe74c1faa716",
      "5c8edbdac53e4208a2b812f0b969033e",
      "0d52f831894c4bf3a598647736d1ca29",
      "6c6cef52fe724612b521f5d13a2c0fc4",
      "f079b8cb27f04541bec84ed39b387b7a",
      "bceb467678a947479005528c9f5a9122",
      "3cb55c545f3146569abce03700f08096",
      "5626ec1a27e843f6b9c2ead882fde64b",
      "bffe7d90a4464ade9508878a36e926bd",
      "277d0b9d28a0445b98f2e6cdb6ef6892",
      "579efaecdf3e42e79163a3a91895fd48",
      "3692b41f934c4229b0c0c5532b30ee24",
      "25727ead43034fb7a8159129ee80d35e",
      "4698876b0de144dbbf2e0a78f614cebf",
      "a4b8244af6b1423e8304de6b2eca4084",
      "d499e8c92a9646f5900c87161f285411",
      "f2062bf200444ead9f3d7f7555f47aa3",
      "cc9e786219d3498a9fab01e71231fda5",
      "6b8d851ac8b1418da211fe09357be446",
      "397c43b52c074f4dafcfcf6810b24e58",
      "14360db516d1493a95f55e5fb85a8ca0",
      "2c0f0b618c8744f589de07167bd3780b",
      "07a17416889f4ef2bd65787dd9cc6eb9",
      "e86bececf8ca4bfbb4e30cf408e3c3ae",
      "c2677e76b394444493640a1b1e030537",
      "dc2fcc3147fe4db6bc4d8ee43307bbf0",
      "65e6ca3d277747e69a612fa321747576",
      "2aae028167484810bb56936b50b81634",
      "b2c0884b8c0b457ba36d887217b6219f",
      "871ec9948de244689dce5f64af54e42c",
      "c43970ce24a649ea9ff7c01c0831552d",
      "74b9226d9cce49639b1a9a83a3e112a0",
      "b804b29717b543299b6176ea12a32bfb",
      "5208691ac1dc460299c29b6492e03f36",
      "e934d7fa6a9a4d55ba7b3a36d054a8b7",
      "98275ace4f7643e29bebbb6d728312f8",
      "1f259ee66f8943b8a70379f3ffbd5ded",
      "17b3d52cccbf4ff7b700c2203a48de3a",
      "76699ff51e074a91b368a0c6ff47738b",
      "a413f1d99746417290eb25a4ff75281f",
      "fd0c649047034feb9e18f75e92183195",
      "ccb28b01548a460cbd1b66ce2c40fede",
      "5a205b3a48c846d2b7e602ad1a0afef7",
      "6bd87ed309c34ba8b6a37edd2e5a479e",
      "536a82a6a1a2425ea2cefe814fbaeb80",
      "9c9e0a938c9d461ba9573c1d6a6854a8",
      "a607efe79ca141f4b1c15d375f6822ab",
      "74d6e30654844e96bc646bbacacd2ad6",
      "93215596f9824fcf8a9f82b4c85ea7f0",
      "ace1bedc57fe47ad95791afc313b52d6",
      "b45bbe429cd44b1c9ba57144fa04540d"
     ]
    },
    "executionInfo": {
     "elapsed": 21573,
     "status": "ok",
     "timestamp": 1755089617106,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "7jRJ7Ud-Pijz",
    "outputId": "ac4da85e-e1b8-4e7c-9267-fb1a6658494b"
   },
   "outputs": [],
   "source": [
    "titles = df_labeled[\"contract_id\"].unique()\n",
    "train_t, test_t = train_test_split(titles, test_size=0.20, random_state=42)\n",
    "train_t, val_t  = train_test_split(train_t, test_size=0.20, random_state=42)\n",
    "\n",
    "def take(ids): return df_labeled[df_labeled[\"contract_id\"].isin(ids)].reset_index(drop=True)\n",
    "train_df, val_df, test_df = take(train_t), take(val_t), take(test_t)\n",
    "\n",
    "MODEL_NAME  = \"bert-base-uncased\"\n",
    "MAX_LEN     = 512\n",
    "BATCH_SIZE  = 8\n",
    "NUM_LABELS  = len(label_names)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "train_ds = ClauseDataset(train_df[\"clause_text\"], train_df[\"y_multi\"], tokenizer, MAX_LEN, NUM_LABELS)\n",
    "val_ds   = ClauseDataset(val_df[\"clause_text\"],   val_df[\"y_multi\"],   tokenizer, MAX_LEN, NUM_LABELS)\n",
    "test_ds  = ClauseDataset(test_df[\"clause_text\"],  test_df[\"y_multi\"],  tokenizer, MAX_LEN, NUM_LABELS)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "Y_train_mat = np.array(train_df[\"y_multi\"].tolist())\n",
    "P = Y_train_mat.sum(axis=0)\n",
    "N = len(Y_train_mat)\n",
    "pos_w_np = (N - P) / np.clip(P, 1, None)\n",
    "pos_w = torch.tensor(pos_w_np, dtype=torch.float32, device=device)\n",
    "bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_w, reduction=\"none\")\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    loss = bce_loss(logits, targets)\n",
    "    return loss.mean()\n",
    "\n",
    "CKPT_DIR = \"/content/drive/MyDrive/data_cuad_transformer/ckpt_bert_finetuned\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1755089625549,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "kQUfkOm8PzyM"
   },
   "outputs": [],
   "source": [
    "def multilabel_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"micro_precision\": precision_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"micro_recall\": recall_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"macro_precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"macro_recall\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "def find_best_threshold_probs(y_true, y_prob, metric_key=\"micro_f1\"):\n",
    "    best_t, best_stats = 0.5, None\n",
    "    for t in np.linspace(0.1, 0.9, 17):\n",
    "        y_hat = (y_prob >= t).astype(int)\n",
    "        stats = multilabel_metrics(y_true, y_hat)\n",
    "        if best_stats is None or stats[metric_key] > best_stats[metric_key]:\n",
    "            best_t, best_stats = float(t), stats\n",
    "    return best_t, best_stats\n",
    "\n",
    "def infer_loader(model, loader):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(**inputs).logits\n",
    "            all_logits.append(logits.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    return np.vstack(all_logits), np.vstack(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 702677,
     "status": "ok",
     "timestamp": 1755090335276,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "_HKaneCzQGOF",
    "outputId": "cbaadd8d-e9d9-4028-a617-983507efbd7e"
   },
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "best_val_micro_f1 = -1.0\n",
    "no_improve = 0\n",
    "patience = 3\n",
    "\n",
    "\n",
    "print(\"Training : \")\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(**inputs).logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    val_logits, y_val = infer_loader(model, val_loader)\n",
    "    val_probs = 1.0 / (1.0 + np.exp(-val_logits))\n",
    "    t_val, stats_val = find_best_threshold_probs(y_val, val_probs)\n",
    "\n",
    "    print(f\"Epoch {epoch} | TrainLoss: {avg_loss:.4f} | Val Micro-F1: {stats_val['micro_f1']:.4f} | Threshold: {t_val:.2f}\")\n",
    "\n",
    "    if stats_val[\"micro_f1\"] > best_val_micro_f1:\n",
    "        best_val_micro_f1 = stats_val[\"micro_f1\"]\n",
    "        model.save_pretrained(CKPT_DIR)\n",
    "        tokenizer.save_pretrained(CKPT_DIR)\n",
    "        with open(os.path.join(CKPT_DIR, \"threshold.json\"), \"w\") as f:\n",
    "            json.dump({\"best_threshold\": float(t_val), \"val_stats\": {k: float(v) for k, v in stats_val.items()}}, f, indent=2)\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9845,
     "status": "ok",
     "timestamp": 1755090810526,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "AZOv6W5Be7Kz",
    "outputId": "9d066242-4100-4981-c5e4-6cfaa8179b52"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(CKPT_DIR, \"threshold.json\"), \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "best_t = float(meta[\"best_threshold\"])\n",
    "best_val_stats = {k: float(v) for k, v in meta[\"val_stats\"].items()}\n",
    "\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(CKPT_DIR).to(device)\n",
    "\n",
    "test_logits, y_test = infer_loader(best_model, test_loader)\n",
    "test_probs = 1.0 / (1.0 + np.exp(-test_logits))\n",
    "y_test_hat = (test_probs >= best_t).astype(int)\n",
    "\n",
    "test_stats = multilabel_metrics(y_test, y_test_hat)\n",
    "\n",
    "print(\"BERT F1 Score (Micro):\", f1_score(y_test, y_test_hat, average=\"micro\", zero_division=0))\n",
    "print(\"VAL :\", {k: round(v, 4) for k, v in best_val_stats.items()})\n",
    "print(\"TEST:\", {k: round(v, 4) for k, v in test_stats.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfXDx0WClBIn"
   },
   "source": [
    "### Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1755091071627,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "5n8MQJ3Wk9ER",
    "outputId": "25f343e8-a0e3-49f0-e4cd-a7056176be0f"
   },
   "outputs": [],
   "source": [
    "def names_from_vec(vec, names):\n",
    "    return [names[i] for i, v in enumerate(vec) if int(v) == 1]\n",
    "\n",
    "np.random.seed(42)\n",
    "k = 3\n",
    "idxs = np.random.choice(len(test_df), size=min(k, len(test_df)), replace=False)\n",
    "\n",
    "for idx in idxs:\n",
    "    clause = test_df[\"clause_text\"].iloc[idx]\n",
    "    clause_short = (clause[:600] + \"...\") if len(clause) > 600 else clause\n",
    "\n",
    "    true_labels = names_from_vec(y_test[idx], label_names)\n",
    "    pred_labels = names_from_vec(y_test_hat[idx], label_names)\n",
    "\n",
    "    print(\"\\n— Clause —\")\n",
    "    print(clause_short, \"\\n\")\n",
    "    print(\" True:\", true_labels)\n",
    "    print(\" Pred (BERT):\", pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkgYpN4jkUpp"
   },
   "source": [
    "### RoBertA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1755092242249,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "vx6mAJtETBcq",
    "outputId": "7a755438-2c5d-42cf-a12d-9ad59477b544"
   },
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "df_labeled = pd.read_pickle(\"/content/drive/MyDrive/data_cuad_transformer/df_labeled.pkl\")\n",
    "import pickle\n",
    "with open(\"/content/drive/MyDrive/data_cuad_transformer/label_names.pkl\", \"rb\") as f:\n",
    "    label_names = pickle.load(f)\n",
    "\n",
    "drop_idx = 9\n",
    "label_names = [lab for i, lab in enumerate(label_names) if i != drop_idx]\n",
    "TARGET_L = len(label_names)\n",
    "\n",
    "def to_fixed(v, L=TARGET_L):\n",
    "    a = np.asarray(v, dtype=np.float32).ravel()\n",
    "    if a.shape[0] >= L:\n",
    "        return a[:L].tolist()\n",
    "    else:\n",
    "        return np.pad(a, (0, L - a.shape[0]), constant_values=0).tolist()\n",
    "\n",
    "df_labeled[\"y_multi\"] = df_labeled[\"y_multi\"].apply(to_fixed)\n",
    "lens = df_labeled[\"y_multi\"].map(len).value_counts()\n",
    "\n",
    "\n",
    "class ClauseDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512, num_labels=None):\n",
    "        self.texts = list(texts)\n",
    "        L = num_labels if num_labels is not None else TARGET_L\n",
    "        self.labels = np.stack([to_fixed(y, L) for y in labels], axis=0).astype(np.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSoAEXvhWlG8"
   },
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1755092254193,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "U8lvoagnWTw6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "titles = df_labeled[\"contract_id\"].unique()\n",
    "train_t, test_t = train_test_split(titles, test_size=0.2, random_state=42)\n",
    "train_t, val_t  = train_test_split(train_t, test_size=0.2, random_state=42)\n",
    "\n",
    "def take(ids): return df_labeled[df_labeled[\"contract_id\"].isin(ids)].reset_index(drop=True)\n",
    "train_df, val_df, test_df = take(train_t), take(val_t), take(test_t)\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_LABELS = len(label_names)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "train_ds = ClauseDataset(train_df[\"clause_text\"], train_df[\"y_multi\"], tokenizer, MAX_LEN)\n",
    "val_ds   = ClauseDataset(val_df[\"clause_text\"], val_df[\"y_multi\"], tokenizer, MAX_LEN)\n",
    "test_ds  = ClauseDataset(test_df[\"clause_text\"], test_df[\"y_multi\"], tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "class ClassBalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, beta=0.9999, gamma=2.0, class_counts=None, device='cpu'):\n",
    "        super().__init__()\n",
    "        effective_num = 1.0 - torch.pow(beta, class_counts)\n",
    "        weights = (1.0 - beta) / (effective_num + 1e-8)\n",
    "        weights = weights / weights.sum() * len(class_counts)\n",
    "        self.weights = weights.to(device)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        loss = ((1 - p_t) ** self.gamma) * ce_loss * self.weights\n",
    "        return loss.mean()\n",
    "\n",
    "CKPT_DIR = \"/content/drive/MyDrive/data_cuad_transformer/ckpt_roberta_finetuned\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "Y_train_mat = np.array(train_df[\"y_multi\"].tolist())\n",
    "class_counts = Y_train_mat.sum(axis=0)\n",
    "loss_fn = ClassBalancedFocalLoss(class_counts=torch.tensor(class_counts), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8Yeq9-sWzq5"
   },
   "source": [
    "### Fine tuning RoBertA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1755092266071,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "IfMo-_XgWyzB",
    "outputId": "944979c3-892e-400e-882e-7747ef6e842b"
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS, problem_type=\"multi_label_classification\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXXQ3Om0XTxR"
   },
   "source": [
    "### Evaluation and threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1755092270963,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "SMRJPiq3XOqZ"
   },
   "outputs": [],
   "source": [
    "def multilabel_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"micro_precision\": precision_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"micro_recall\": recall_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"macro_precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"macro_recall\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "def find_best_threshold_probs(y_true, y_prob, metric_key=\"micro_f1\"):\n",
    "    best_t, best_stats = 0.5, None\n",
    "    for t in np.linspace(0.1, 0.9, 17):\n",
    "        y_hat = (y_prob >= t).astype(int)\n",
    "        stats = multilabel_metrics(y_true, y_hat)\n",
    "        if best_stats is None or stats[metric_key] > best_stats[metric_key]:\n",
    "            best_t, best_stats = float(t), stats\n",
    "    return best_t, best_stats\n",
    "\n",
    "def infer_loader(model, loader):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(**inputs).logits\n",
    "            all_logits.append(logits.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    return np.vstack(all_logits), np.vstack(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Deziy_F6XopJ"
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 683213,
     "status": "ok",
     "timestamp": 1755092958722,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "HSzzV_qtXntN",
    "outputId": "101803dc-8643-4a51-c2e6-d8e721b666f7"
   },
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "best_val_micro_f1 = -1.0\n",
    "no_improve = 0\n",
    "patience = 3\n",
    "\n",
    "print(\"Training : \")\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(**inputs).logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    val_logits, y_val = infer_loader(model, val_loader)\n",
    "    val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "    t_val, stats_val = find_best_threshold_probs(y_val, val_probs)\n",
    "\n",
    "    print(f\"Epoch {epoch} | Loss: {avg_loss:.4f} | Val Micro-F1: {stats_val['micro_f1']:.4f} | Threshold: {t_val:.2f}\")\n",
    "\n",
    "    if stats_val[\"micro_f1\"] > best_val_micro_f1:\n",
    "        best_val_micro_f1 = stats_val[\"micro_f1\"]\n",
    "        model.save_pretrained(CKPT_DIR)\n",
    "        tokenizer.save_pretrained(CKPT_DIR)\n",
    "        with open(os.path.join(CKPT_DIR, \"threshold.json\"), \"w\") as f:\n",
    "            json.dump({\"best_threshold\": float(t_val), \"val_stats\": stats_val}, f, indent=2)\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12759,
     "status": "ok",
     "timestamp": 1755093044758,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "bb4QIzZJXxMf",
    "outputId": "cde17a44-3c00-4132-9f2f-34cfbea04288"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "with open(os.path.join(CKPT_DIR, \"threshold.json\"), \"r\") as f:\n",
    "    meta_rb = json.load(f)\n",
    "\n",
    "best_t_rb = float(meta_rb[\"best_threshold\"])\n",
    "best_val_stats_rb = {k: float(v) for k, v in meta_rb[\"val_stats\"].items()}\n",
    "\n",
    "roberta_best = AutoModelForSequenceClassification.from_pretrained(CKPT_DIR).to(device)\n",
    "\n",
    "test_logits_rb, y_test = infer_loader(roberta_best, test_loader)\n",
    "test_probs_rb = 1.0 / (1.0 + np.exp(-test_logits_rb))\n",
    "y_test_hat_roberta = (test_probs_rb >= best_t_rb).astype(int)\n",
    "\n",
    "test_stats_rb = multilabel_metrics(y_test, y_test_hat_roberta)\n",
    "\n",
    "print(\"RoBERTa F1 Score (Micro):\", f1_score(y_test, y_test_hat_roberta, average=\"micro\", zero_division=0))\n",
    "print(\"VAL :\", {k: round(v, 4) for k, v in best_val_stats_rb.items()})\n",
    "print(\"TEST:\", {k: round(v, 4) for k, v in test_stats_rb.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTaPGsb8rHdP"
   },
   "source": [
    "### Sample Output :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1755093122022,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "lYstXMxmrMFN",
    "outputId": "89b957fd-ab9b-4d09-e82c-fb72622eba03"
   },
   "outputs": [],
   "source": [
    "def names_from_vec(vec, names):\n",
    "    return [names[i] for i, v in enumerate(vec) if int(v) == 1]\n",
    "\n",
    "def show_roberta_preds(k=3, seed=42):\n",
    "\n",
    "    y_hat_roberta = globals().get(\"y_test_hat_roberta\", globals().get(\"y_test_hat\"))\n",
    "    assert y_hat_roberta is not None, \"Run RoBERTa eval first to create y_test_hat (or y_test_hat_roberta).\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    k = min(k, len(test_df))\n",
    "    idxs = np.random.choice(len(test_df), size=k, replace=False)\n",
    "\n",
    "    for idx in idxs:\n",
    "        clause = test_df[\"clause_text\"].iloc[idx]\n",
    "        clause_short = (clause[:600] + \"...\") if len(clause) > 600 else clause\n",
    "\n",
    "        true_labels = names_from_vec(y_test[idx], label_names)\n",
    "        pred_labels = names_from_vec(y_hat_roberta[idx], label_names)\n",
    "\n",
    "        print(\"\\n— Clause —\")\n",
    "        print(clause_short, \"\\n\")\n",
    "        print(\" True:\", true_labels)\n",
    "        print(\" Pred (RoBERTa):\", pred_labels)\n",
    "\n",
    "show_roberta_preds(k=3, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9WsohdVdzVz"
   },
   "source": [
    "### Legal Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1755093456905,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "5c7fOY2JYe_v",
    "outputId": "866cbd15-087c-4802-e9b4-7499027928e2"
   },
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "df_labeled = pd.read_pickle(\"/content/drive/MyDrive/data_cuad_transformer/df_labeled.pkl\")\n",
    "import pickle\n",
    "with open(\"/content/drive/MyDrive/data_cuad_transformer/label_names.pkl\", \"rb\") as f:\n",
    "    label_names = pickle.load(f)\n",
    "\n",
    "df_labeled[\"y_multi\"] = df_labeled[\"y_multi\"].apply(lambda v: np.asarray(v, dtype=np.float32))\n",
    "y_lengths = df_labeled[\"y_multi\"].map(lambda v: v.shape[0]).unique()\n",
    "if len(y_lengths) != 1:\n",
    "    raise ValueError(f\"Inconsistent y_multi lengths across rows: {y_lengths}\")\n",
    "\n",
    "TRUE_NUM_LABELS = int(y_lengths[0])\n",
    "\n",
    "if len(label_names) != TRUE_NUM_LABELS:\n",
    "    print(f\"label_names len={len(label_names)}\")\n",
    "    if len(label_names) > TRUE_NUM_LABELS:\n",
    "        label_names = list(label_names)[:TRUE_NUM_LABELS]\n",
    "    else:\n",
    "        label_names = list(label_names) + [f\"label_{i}\" for i in range(len(label_names), TRUE_NUM_LABELS)]\n",
    "\n",
    "NUM_LABELS = TRUE_NUM_LABELS\n",
    "\n",
    "def _align_to_n(vec, n):\n",
    "    vec = np.asarray(vec, dtype=np.float32)\n",
    "    if vec.ndim == 1:\n",
    "        if vec.shape[0] > n:\n",
    "            return vec[:n]\n",
    "        if vec.shape[0] < n:\n",
    "            return np.pad(vec, (0, n - vec.shape[0]), constant_values=0.0)\n",
    "        return vec\n",
    "    if vec.shape[1] > n:\n",
    "        return vec[:, :n]\n",
    "    if vec.shape[1] < n:\n",
    "        pad = np.zeros((vec.shape[0], n - vec.shape[1]), dtype=np.float32)\n",
    "        return np.hstack([vec, pad])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1755093459198,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "funuiZq4d626"
   },
   "outputs": [],
   "source": [
    "class ClauseDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512, num_labels=None):\n",
    "        self.texts = list(texts)\n",
    "        arr = np.array(list(labels), dtype=np.float32)\n",
    "        if num_labels is not None:\n",
    "            arr = _align_to_n(arr, num_labels)\n",
    "        self.labels = arr\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUu-2uOrrq0y"
   },
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1755093533185,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "J1dIYU1jebTs",
    "outputId": "0e3dcc62-1433-4fa1-bb00-162248bfa2c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "titles = df_labeled[\"contract_id\"].unique()\n",
    "train_t, test_t = train_test_split(titles, test_size=0.2, random_state=42)\n",
    "train_t, val_t  = train_test_split(train_t, test_size=0.2, random_state=42)\n",
    "\n",
    "def take(ids): return df_labeled[df_labeled[\"contract_id\"].isin(ids)].reset_index(drop=True)\n",
    "train_df, val_df, test_df = take(train_t), take(val_t), take(test_t)\n",
    "\n",
    "MODEL_NAME = \"nlpaueb/legal-bert-base-uncased\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_LABELS = len(label_names)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "train_ds = ClauseDataset(train_df[\"clause_text\"], train_df[\"y_multi\"], tokenizer, MAX_LEN, num_labels=NUM_LABELS)\n",
    "val_ds   = ClauseDataset(val_df[\"clause_text\"],   val_df[\"y_multi\"],   tokenizer, MAX_LEN, num_labels=NUM_LABELS)\n",
    "test_ds  = ClauseDataset(test_df[\"clause_text\"],  test_df[\"y_multi\"],  tokenizer, MAX_LEN, num_labels=NUM_LABELS)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n",
    "\n",
    "class ClassBalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, beta=0.9999, gamma=2.0, class_counts=None, device='cpu'):\n",
    "        super().__init__()\n",
    "        effective_num = 1.0 - torch.pow(beta, class_counts)\n",
    "        weights = (1.0 - beta) / (effective_num + 1e-8)\n",
    "        weights = weights / weights.sum() * len(class_counts)\n",
    "        self.weights = weights.to(device)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        loss = ((1 - p_t) ** self.gamma) * ce_loss * self.weights\n",
    "        return loss.mean()\n",
    "\n",
    "CKPT_DIR = \"/content/drive/MyDrive/data_cuad_transformer/ckpt_legalbert_finetuned\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "Y_train_mat = np.array(list(train_df[\"y_multi\"]), dtype=np.float32)\n",
    "Y_train_mat = _align_to_n(Y_train_mat, NUM_LABELS)\n",
    "class_counts = Y_train_mat.sum(axis=0)\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS, problem_type=\"multi_label_classification\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IvnaQ2pr1R7"
   },
   "source": [
    "## Metrics and Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1755093535458,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "UitT-8Uvekg8"
   },
   "outputs": [],
   "source": [
    "def multilabel_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"micro_precision\": precision_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"micro_recall\": recall_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"macro_precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"macro_recall\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "def find_best_threshold_probs(y_true, y_prob, metric_key=\"micro_f1\"):\n",
    "    best_t, best_stats = 0.5, None\n",
    "    for t in np.linspace(0.1, 0.9, 17):\n",
    "        y_hat = (y_prob >= t).astype(int)\n",
    "        stats = multilabel_metrics(y_true, y_hat)\n",
    "        if best_stats is None or stats[metric_key] > best_stats[metric_key]:\n",
    "            best_t, best_stats = float(t), stats\n",
    "    return best_t, best_stats\n",
    "\n",
    "def infer_loader(model, loader):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(**inputs).logits\n",
    "            all_logits.append(logits.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    return np.vstack(all_logits), np.vstack(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C_73zWcr_bY"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 628768,
     "status": "ok",
     "timestamp": 1755094166386,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "j9-T-PrtesFW",
    "outputId": "c8e038d3-05a2-4d9b-d6d3-9d9b9568edb9"
   },
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "best_val_micro_f1 = -1.0\n",
    "no_improve = 0\n",
    "patience = 3\n",
    "\n",
    "print(\"Training :\")\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(**inputs).logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    val_logits, y_val = infer_loader(model, val_loader)\n",
    "    val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "    t_val, stats_val = find_best_threshold_probs(y_val, val_probs)\n",
    "\n",
    "    print(f\"Epoch {epoch} | Loss: {avg_loss:.4f} | Val Micro-F1: {stats_val['micro_f1']:.4f} | Threshold: {t_val:.2f}\")\n",
    "\n",
    "    if stats_val[\"micro_f1\"] > best_val_micro_f1:\n",
    "        best_val_micro_f1 = stats_val[\"micro_f1\"]\n",
    "        model.save_pretrained(CKPT_DIR)\n",
    "        tokenizer.save_pretrained(CKPT_DIR)\n",
    "        with open(os.path.join(CKPT_DIR, \"threshold.json\"), \"w\") as f:\n",
    "            json.dump({\"best_threshold\": float(t_val), \"val_stats\": stats_val}, f, indent=2)\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9oybmr6sGXz"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14383,
     "status": "ok",
     "timestamp": 1755094379527,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "45XL_FVwev0f",
    "outputId": "3f1dd431-746f-47bd-e4f4-3d97226462d1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "with open(os.path.join(CKPT_DIR, \"threshold.json\"), \"r\") as f:\n",
    "    meta_lb = json.load(f)\n",
    "\n",
    "best_t_lb = float(meta_lb[\"best_threshold\"])\n",
    "best_val_stats_lb = {k: float(v) for k, v in meta_lb[\"val_stats\"].items()}\n",
    "\n",
    "legalbert_best = AutoModelForSequenceClassification.from_pretrained(CKPT_DIR).to(device)\n",
    "legalbert_best.eval()\n",
    "\n",
    "test_logits_lb, y_test = infer_loader(legalbert_best, test_loader)\n",
    "test_probs_lb = 1.0 / (1.0 + np.exp(-test_logits_lb))\n",
    "y_test_hat_legalbert = (test_probs_lb >= best_t_lb).astype(int)\n",
    "\n",
    "test_stats_lb = multilabel_metrics(y_test, y_test_hat_legalbert)\n",
    "\n",
    "print(\"LegalBERT F1 Score (Micro):\", f1_score(y_test, y_test_hat_legalbert, average=\"micro\", zero_division=0))\n",
    "print(\"VAL :\", {k: round(v, 4) for k, v in best_val_stats_lb.items()})\n",
    "print(\"TEST:\", {k: round(v, 4) for k, v in test_stats_lb.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma-Tbn666emQ"
   },
   "source": [
    "## Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1755096845456,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "Q8Ixy3Ld6gl3",
    "outputId": "885093d3-c478-4d3d-dd35-ce5f327d50bd"
   },
   "outputs": [],
   "source": [
    "def align_cols(A, N):\n",
    "    A = np.asarray(A)\n",
    "    if A.ndim == 1:\n",
    "        A = A[None, :]\n",
    "    if A.shape[1] > N:\n",
    "        return A[:, :N]\n",
    "    if A.shape[1] < N:\n",
    "        pad = np.zeros((A.shape[0], N - A.shape[1]), dtype=A.dtype)\n",
    "        return np.hstack([A, pad])\n",
    "    return A\n",
    "def names_from_vec(vec, names):\n",
    "    vec = np.asarray(vec).ravel()\n",
    "    N = min(len(vec), len(names))\n",
    "    return [names[i] for i in range(N) if int(vec[i]) == 1]\n",
    "\n",
    "N = len(label_names)\n",
    "y_test_arr = align_cols(y_test, N).astype(int)\n",
    "y_pred_legalbert = align_cols(y_test_hat, N).astype(int)\n",
    "\n",
    "np.random.seed(42)\n",
    "k = 3\n",
    "idxs = np.random.choice(len(test_df), size=min(k, len(test_df)), replace=False)\n",
    "\n",
    "for idx in idxs:\n",
    "    clause = test_df[\"clause_text\"].iloc[idx]\n",
    "    clause_short = (clause[:600] + \"...\") if len(clause) > 600 else clause\n",
    "\n",
    "    true_labels = names_from_vec(y_test_arr[idx], label_names)\n",
    "    pred_labels = names_from_vec(y_pred_legalbert[idx], label_names)\n",
    "\n",
    "    print(\"\\n— Clause —\")\n",
    "    print(clause_short, \"\\n\")\n",
    "    print(\" True:\", true_labels)\n",
    "    print(\" Pred (LegalBERT):\", pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 32489,
     "status": "ok",
     "timestamp": 1755097146807,
     "user": {
      "displayName": "3012 DEVADARSHINI P.T",
      "userId": "12107410435582174408"
     },
     "user_tz": 240
    },
    "id": "mZSuwCyN61Re",
    "outputId": "f4d7d2c3-dc7a-48d3-b578-032e2c05af82"
   },
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ClauseDatasetSimple(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512, num_labels=None):\n",
    "        self.texts = list(texts)\n",
    "        arr = np.array(list(labels), dtype=np.float32)\n",
    "        if num_labels is not None:\n",
    "            if arr.ndim == 1:\n",
    "                arr = arr[None, :]\n",
    "            if arr.shape[1] > num_labels:\n",
    "                arr = arr[:, :num_labels]\n",
    "            elif arr.shape[1] < num_labels:\n",
    "                pad = np.zeros((arr.shape[0], num_labels - arr.shape[1]), dtype=np.float32)\n",
    "                arr = np.hstack([arr, pad])\n",
    "        self.labels = arr\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx], truncation=True, padding=\"max_length\",\n",
    "            max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "def infer_loader(model, loader):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(**inputs).logits\n",
    "            all_logits.append(logits.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    return np.vstack(all_logits), np.vstack(all_labels)\n",
    "\n",
    "def evaluate_ckpt(ckpt_dir, model_name, max_len=512, batch_size=8):\n",
    "    \"\"\"Load tokenizer+model from ckpt_dir, rebuild test loader, evaluate with saved threshold.\"\"\"\n",
    "    thresh_path = os.path.join(ckpt_dir, \"threshold.json\")\n",
    "    if not (os.path.isdir(ckpt_dir) and os.path.isfile(thresh_path)):\n",
    "        return None, f\"[skip] Missing checkpoint or threshold.json for {model_name} at {ckpt_dir}\"\n",
    "    tok = AutoTokenizer.from_pretrained(ckpt_dir, use_fast=True)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(ckpt_dir).to(device)\n",
    "\n",
    "    num_labels = mdl.config.num_labels\n",
    "    test_ds = ClauseDatasetSimple(\n",
    "        test_df[\"clause_text\"], test_df[\"y_multi\"],\n",
    "        tokenizer=tok, max_len=max_len, num_labels=num_labels\n",
    "    )\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    with open(thresh_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    best_t = float(meta.get(\"best_threshold\", 0.5))\n",
    "    val_stats = {k: float(v) for k, v in meta.get(\"val_stats\", {}).items()}\n",
    "\n",
    "\n",
    "    test_logits, y_test = infer_loader(mdl, test_loader)\n",
    "    test_probs = 1.0 / (1.0 + np.exp(-test_logits))\n",
    "    y_hat = (test_probs >= best_t).astype(int)\n",
    "    test_stats = multilabel_metrics(y_test, y_hat)\n",
    "\n",
    "    row = {\n",
    "        \"model\": model_name,\n",
    "        \"n_labels\": int(num_labels),\n",
    "        \"best_t\": round(best_t, 3),\n",
    "        \"val_micro_f1\": round(val_stats.get(\"micro_f1\", float(\"nan\")), 4),\n",
    "        \"val_macro_f1\": round(val_stats.get(\"macro_f1\", float(\"nan\")), 4),\n",
    "        \"test_micro_f1\": round(test_stats[\"micro_f1\"], 4),\n",
    "        \"test_macro_f1\": round(test_stats[\"macro_f1\"], 4),\n",
    "        \"test_precision\": round(test_stats[\"micro_precision\"], 4),\n",
    "        \"test_recall\": round(test_stats[\"micro_recall\"], 4),\n",
    "    }\n",
    "    return row, None\n",
    "\n",
    "CKPTS = [\n",
    "    (\"RoBERTa\",   \"/content/drive/MyDrive/data_cuad_transformer/ckpt_roberta_finetuned\"),\n",
    "    (\"LegalBERT\", \"/content/drive/MyDrive/data_cuad_transformer/ckpt_legalbert_finetuned\"),\n",
    "     (\"BERT\",      \"/content/drive/MyDrive/data_cuad_transformer/ckpt_bert_finetuned\"),\n",
    "]\n",
    "\n",
    "rows, notes = [], []\n",
    "for name, path in CKPTS:\n",
    "    r, msg = evaluate_ckpt(path, name, max_len=512, batch_size=8)\n",
    "    if r is not None:\n",
    "        rows.append(r)\n",
    "    if msg:\n",
    "        notes.append(msg)\n",
    "\n",
    "if rows:\n",
    "    results_df = (pd.DataFrame(rows)\n",
    "                    .sort_values(\"test_micro_f1\", ascending=False)\n",
    "                    .reset_index(drop=True))\n",
    "    display(results_df)\n",
    "    print(f\"\\nBest: {results_df.iloc[0]['model']} | test micro-F1={results_df.iloc[0]['test_micro_f1']:.4f} | n_labels={results_df.iloc[0]['n_labels']} | best_t={results_df.iloc[0]['best_t']:.3f}\")\n",
    "else:\n",
    "    print(\"No models evaluated. Check CKPT paths/threshold.json files.\")\n",
    "\n",
    "for n in notes:\n",
    "    print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXHkiKu28KXy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN8Ba2P7DqJXquSWv9J+h0J",
   "gpuType": "T4",
   "mount_file_id": "1K35dvppZG5EEPZoUcQN6cKaXGhbR0vFZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
